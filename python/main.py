#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åä¸ºå¼€å‘è€…æ–‡æ¡£æŠ“å–è„šæœ¬
ä½¿ç”¨ Playwright æŠ“å–åŠ¨æ€ç½‘é¡µå†…å®¹å¹¶ä¿å­˜ä¸º Markdown æ ¼å¼
"""

import asyncio
import os
import re
from pathlib import Path
from urllib.parse import urlparse
from playwright.async_api import async_playwright, Page
import html2text


class HuaweiDocScraper:
    """åä¸ºå¼€å‘è€…æ–‡æ¡£æŠ“å–å™¨"""

    def __init__(self, output_dir: str = "."):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.ignore_emphasis = False
        self.h2t.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
        self.h2t.unicode_snob = True
        self.h2t.skip_internal_links = False

    def sanitize_filename(self, title: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        title = re.sub(r'[<>:"/\\|?*]', '_', title)
        # ç§»é™¤å‰åç©ºæ ¼
        title = title.strip()
        # é™åˆ¶é•¿åº¦
        if len(title) > 200:
            title = title[:200]
        return title

    async def wait_for_content_load(self, page: Page):
        """ç­‰å¾…é¡µé¢å†…å®¹å®Œå…¨åŠ è½½"""
        try:
            # ç­‰å¾…ä¸»è¦å†…å®¹åŒºåŸŸåŠ è½½
            await page.wait_for_selector('article, .content, .doc-content, main', timeout=10000)
            # ç­‰å¾…ç½‘ç»œç©ºé—²
            await page.wait_for_load_state('networkidle', timeout=15000)
            # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹æ¸²æŸ“
            await asyncio.sleep(2)
        except Exception as e:
            print(f"ç­‰å¾…å†…å®¹åŠ è½½æ—¶å‡ºç°è­¦å‘Š: {e}")
            # å³ä½¿è¶…æ—¶ä¹Ÿç»§ç»­ï¼Œå› ä¸ºéƒ¨åˆ†å†…å®¹å¯èƒ½å·²ç»åŠ è½½

    async def extract_content(self, page: Page) -> tuple[str, str]:
        """æå–é¡µé¢æ ‡é¢˜å’Œä¸»è¦å†…å®¹"""
        # è·å–æ ‡é¢˜
        title = await page.title()
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            'article',
            '.doc-content',
            '.content',
            'main',
            '#content',
            '.markdown-body',
            '[role="main"]'
        ]
        
        content_html = None
        for selector in content_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    content_html = await element.inner_html()
                    if content_html and len(content_html.strip()) > 100:
                        break
            except Exception:
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œä½¿ç”¨æ•´ä¸ª body
        if not content_html:
            body = await page.query_selector('body')
            if body:
                content_html = await body.inner_html()
        
        return title, content_html or ""

    async def scrape_page(self, page: Page, url: str) -> dict:
        """æŠ“å–å•ä¸ªé¡µé¢"""
        print(f"æ­£åœ¨æŠ“å–: {url}")
        
        try:
            # è®¿é—®é¡µé¢
            response = await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            if not response or response.status >= 400:
                print(f"âš ï¸  é¡µé¢åŠ è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status if response else 'unknown'})")
                return None
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            await self.wait_for_content_load(page)
            
            # æå–å†…å®¹
            title, content_html = await self.extract_content(page)
            
            # è½¬æ¢ä¸º Markdown
            markdown_content = self.h2t.handle(content_html)
            
            # æ·»åŠ å…ƒæ•°æ®
            metadata = f"# {title}\n\n"
            metadata += f"**æºåœ°å€**: {url}\n\n"
            metadata += f"---\n\n"
            
            full_content = metadata + markdown_content
            
            return {
                'url': url,
                'title': title,
                'content': full_content
            }
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ {url}: {str(e)}")
            return None

    def save_markdown(self, data: dict) -> str:
        """ä¿å­˜ Markdown æ–‡ä»¶"""
        if not data:
            return None
        
        # ä» URL æå–æ–‡ä»¶å
        url_path = urlparse(data['url']).path
        url_filename = url_path.split('/')[-1] or 'index'
        
        # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæ–‡ä»¶åï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if data['title']:
            filename = self.sanitize_filename(data['title'])
        else:
            filename = url_filename
        
        # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€æ€§
        filepath = self.output_dir / f"{filename}.md"
        counter = 1
        while filepath.exists():
            filepath = self.output_dir / f"{filename}_{counter}.md"
            counter += 1
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(data['content'])
        
        print(f"âœ… å·²ä¿å­˜: {filepath}")
        return str(filepath)

    async def scrape_urls(self, urls: list[str]):
        """æ‰¹é‡æŠ“å–å¤šä¸ª URL"""
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨ï¼ˆä½¿ç”¨ headless æ¨¡å¼ï¼‰
            browser = await p.chromium.launch(headless=True)
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œè®¾ç½®ç”¨æˆ·ä»£ç†
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            
            results = []
            for url in urls:
                try:
                    data = await self.scrape_page(page, url)
                    if data:
                        filepath = self.save_markdown(data)
                        results.append(filepath)
                    # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    await asyncio.sleep(1)
                except Exception as e:
                    print(f"âŒ å¤„ç† {url} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            await browser.close()
            
            return results


async def main():
    """ä¸»å‡½æ•°"""
    # è¦æŠ“å–çš„ URL åˆ—è¡¨
    urls = [
        'https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/jsvm',
        'https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/jsvm-introduction',
        'https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/jsvm-data-types-interfaces',
        'https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/use-jsvm-process',
        'https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/jsvm-guidelines',
        'https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/jsvm-frequently-questions',
    ]
    
    print("ğŸš€ å¼€å§‹æŠ“å–åä¸ºå¼€å‘è€…æ–‡æ¡£...\n")
    
    # åˆ›å»ºæŠ“å–å™¨å®ä¾‹ï¼ˆä¿å­˜åˆ°å½“å‰ç›®å½•ï¼‰
    scraper = HuaweiDocScraper(output_dir=".")
    
    # æ‰§è¡ŒæŠ“å–
    results = await scraper.scrape_urls(urls)
    
    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼å…±æˆåŠŸä¿å­˜ {len(results)} ä¸ªæ–‡ä»¶")
    print(f"æ–‡ä»¶ä¿å­˜ä½ç½®: {os.path.abspath('.')}")


if __name__ == "__main__":
    asyncio.run(main())
